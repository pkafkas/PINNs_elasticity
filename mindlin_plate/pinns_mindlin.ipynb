{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "#from torchinfo import summary\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace(y, x):\n",
    "    grad = gradient(y, x)\n",
    "    return divergence(grad, x)\n",
    "\n",
    "\n",
    "def divergence(y, x):\n",
    "    div = 0.\n",
    "    for i in range(y.shape[-1]):\n",
    "        div += torch.autograd.grad(y[..., i], x, torch.ones_like(y[..., i]), create_graph=True)[0][..., i:i+1]\n",
    "    return div\n",
    "\n",
    "\n",
    "def gradient(y, x, grad_outputs=None):\n",
    "    if grad_outputs is None:\n",
    "        grad_outputs = torch.ones_like(y)\n",
    "    grad = torch.autograd.grad(y, x, grad_outputs=grad_outputs, create_graph=True, allow_unused=True)[0]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedGlorotNormal(nn.Module):\n",
    "    def __init__(self, mean=1.0, stddev=0.1):\n",
    "        super(FactorizedGlorotNormal, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.stddev = torch.tensor(stddev)\n",
    "\n",
    "    def forward(self, shape):\n",
    "        w =  torch.nn.init.xavier_normal_(torch.empty(shape))#.normal_(mean=0, std=1)\n",
    "        s = torch.exp(self.mean + torch.empty((shape[-1],)).normal_(mean=0, std=self.stddev))\n",
    "        v = w / s\n",
    "        return  s, v\n",
    "\n",
    "\n",
    "class FactorizedDense(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(FactorizedDense, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.kernel = FactorizedGlorotNormal()\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "        self.s = nn.Parameter(self.kernel((self.in_features, self.out_features))[0])\n",
    "        self.v = nn.Parameter(self.kernel((self.in_features, self.out_features))[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        #s, v = self.kernel((self.in_features, self.out_features))\n",
    "        kernel = self.s * self.v\n",
    "        y = torch.matmul(x, kernel) + self.bias\n",
    "        return y\n",
    "\n",
    "\n",
    "class FourierFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, sigma=3.0):\n",
    "        super(FourierFeatureEmbedding, self).__init__()\n",
    "        self.B = torch.randn((embedding_dim // 2, input_dim)) * sigma #* 2 * torch.pi\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = 2 * torch.pi * x @ self.B.t()\n",
    "        x_proj = torch.deg2rad(x_proj)\n",
    "        #print(x.shape, self.B.shape, x_proj.shape)\n",
    "        return torch.cat([torch.cos(x_proj), torch.sin(x_proj)], dim=1)\n",
    "\n",
    "\n",
    "class Stanh(nn.Module):\n",
    "    def __init__(self, feat_num):\n",
    "        super(Stanh, self).__init__()\n",
    "        self.a = nn.Parameter(torch.zeros(feat_num))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.tanh(x)  + self.a * x * torch.tanh(x)\n",
    "        return output\n",
    "\n",
    "def dist(x1, y1, x2, y2):\n",
    "    return torch.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "def linseg(x, y, x1, y1, x2, y2):\n",
    "    L = dist(x1, y1, x2, y2)\n",
    "    xc = (x1 + x2) / 2.\n",
    "    yc = (y1 + y2) / 2.\n",
    "    f = (1 / L) * ((x - x1) * (y2 - y1) - (y - y1) * (x2 - x1))\n",
    "    t = (1 / L) * ((L / 2.)**2 - dist(x, y, xc, yc)**2)\n",
    "    varphi = torch.sqrt(t**2 + f**4)\n",
    "    phi = torch.sqrt(f**2 + (1 / 4.) * (varphi - t)**2)\n",
    "    return phi\n",
    "\n",
    "def phi(x, y, segments):\n",
    "    m = 1.\n",
    "    R = 0.\n",
    "    for i in range(segments.size(0)):\n",
    "        x1, y1, x2, y2 = segments[i]\n",
    "        phi_val = linseg(x, y, x1, y1, x2, y2)\n",
    "        R = R + 1. / phi_val**m\n",
    "    R = 1. / R**(1. / m)\n",
    "    return R\n",
    "\n",
    "segments = torch.tensor([[-0.5, -0.5, 0.5, -0.5], [0.5, -0.5, 0.5, 0.5], [0.5, 0.5, -0.5, 0.5], [-0.5, 0.5, -0.5, -0.5]])\n",
    "\n",
    "class Pinn(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_layers, output_dim, sigma=1.0, lr=1e-3, fourier=True):\n",
    "        super(Pinn, self).__init__()\n",
    "        self.fourier = fourier\n",
    "\n",
    "        if self.fourier:\n",
    "            self.fourier_features = FourierFeatureEmbedding(input_dim, embedding_dim, sigma)\n",
    "            input_dim = embedding_dim  \n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.acts = []\n",
    "\n",
    "        current_dim = input_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            self.layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            self.acts.append(Stanh(hidden_dim))\n",
    "            current_dim = hidden_dim\n",
    "\n",
    "        self.layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.layers.apply(self.init_weights)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "      if isinstance(m, nn.Linear):\n",
    "          torch.nn.init.xavier_uniform_(m.weight)\n",
    "          torch.nn.init.normal_(m.bias)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        xy = torch.cat([x,y], dim=1)\n",
    "      \n",
    "        dist = ((xy[:, 0:1] - 0.5) * (xy[:, 0:1] + 0.5) * (xy[:, 1:2] - 0.5) * (xy[:, 1:2] + 0.5))\n",
    "\n",
    "        if self.fourier:\n",
    "            xy = self.fourier_features(xy)\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            xy = self.acts[i](self.layers[i](xy))\n",
    "        xy = self.layers[-1](xy)\n",
    "\n",
    "        w, phix, phiy, phix_x_plus_phiy_y, phix_y_plus_phiy_x = xy.split(1, dim=1)\n",
    "\n",
    "        w = w*dist**2\n",
    "        return torch.cat([w, phix, phiy, phix_x_plus_phiy_y, phix_y_plus_phiy_x], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pde():\n",
    "    def __init__(self, E, nu=0.3, h=0.1, k=5/6):\n",
    "\n",
    "        G = E/(2*(1+nu))\n",
    "\n",
    "        self.h = h\n",
    "        self.k = k\n",
    "\n",
    "        self.D = (E*h**3)/(12*(1-nu**2))\n",
    "        self.kGh = k*G*h\n",
    "        self.Dc = (2*self.kGh)/(self.D*(1-nu))\n",
    "        self.Ds = k*G*h\n",
    "        self.Db = (E*h**3)/(12*(1-nu**2))\n",
    "\n",
    "        print(E, G, self.D, self.Dc)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def lossWeight(self, losses):\n",
    "        with torch.no_grad():\n",
    "          total = 0.0\n",
    "          weights = []\n",
    "          for i in range(len(losses)):\n",
    "            total += i\n",
    "\n",
    "          for i in range(len(losses)):\n",
    "            weight = i/total\n",
    "            weights.append(1-weight)\n",
    "        return weights\n",
    "\n",
    "    def getDerivs(self, w, phix, phiy):\n",
    "\n",
    "        phix_x = gradient(phix, x)\n",
    "        phiy_y = gradient(phiy, y)\n",
    "\n",
    "        phix_y = gradient(phix, y)\n",
    "        phiy_x = gradient(phiy, x)\n",
    "\n",
    "        phix_x_plus_phiy_y = phix_x + phiy_y\n",
    "        phix_y_minus_phiy_x = phix_y - phiy_x\n",
    "\n",
    "        derivs = torch.cat([phix_x_plus_phiy_y, phix_y_minus_phiy_x], dim=1)\n",
    "        return derivs\n",
    "\n",
    "    def getStresses(self, bstrains, sstrains):\n",
    "        bstresses = torch.matmul(self.Df, bstrains.T)\n",
    "        sstresses = torch.matmul(self.Dc, sstrains.T)\n",
    "        return bstresses.T, sstresses.T\n",
    "\n",
    "    def getLoss(self, model, x, y):\n",
    "\n",
    "        output = model(x,y)\n",
    "        w, phix, phiy, phix_x_plus_phiy_y, phix_y_minus_phiy_x = output.split(1, dim=1)\n",
    "\n",
    "        derivs_pred = torch.cat([phix_x_plus_phiy_y, phix_y_minus_phiy_x], dim=1)\n",
    "        target_derivs = self.getDerivs(w, phix, phiy)\n",
    "\n",
    "        derivloss = self.criterion(derivs_pred, target_derivs)\n",
    "\n",
    "        w_lap = laplace(w, x) + laplace(w, y)\n",
    "\n",
    "        q = 1.0\n",
    "\n",
    "        eq1 = (laplace(phix_x_plus_phiy_y, x) + laplace(phix_x_plus_phiy_y, y)) + q/self.D\n",
    "        eq2 = w_lap + phix_x_plus_phiy_y + q/(self.kGh*self.k)\n",
    "        eq3 = (laplace(phix_y_minus_phiy_x, x) + laplace(phix_y_minus_phiy_x,y)) - (12*self.k*2/self.h**2)*(phix_y_minus_phiy_x)\n",
    "\n",
    "\n",
    "        res1 = self.criterion(eq1, torch.zeros_like(eq1))\n",
    "        res2 = self.criterion(eq2, torch.zeros_like(eq2))\n",
    "        res3 = self.criterion(eq3, torch.zeros_like(eq3))\n",
    "        resloss = res1 + res2 + res3\n",
    "\n",
    "        return resloss, derivloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(model, comp):\n",
    "    with torch.no_grad():\n",
    "        x_vals = torch.linspace(-0.5, 0.5, steps=100)\n",
    "        y_vals = torch.linspace(-0.5, 0.5, steps=100)\n",
    "        X, Y = torch.meshgrid(x_vals, y_vals, indexing='xy')\n",
    "        Z = torch.zeros_like(X)\n",
    "\n",
    "        input_data = torch.stack([X.flatten(), Y.flatten(), Z.flatten()], dim=1)\n",
    "        solution_w = model(X.flatten().unsqueeze(1), Y.flatten().unsqueeze(1))[:, 0:1]\n",
    "        solution_w = solution_w.reshape(X.shape)\n",
    "\n",
    "        fem = comp\n",
    "        max_w = torch.abs(solution_w).max().item()\n",
    "        rel_diff = (abs(max_w - fem) / fem) * 100\n",
    "        print(f'Max w: {max_w}, FEM: {fem}, Relative Difference at center: {rel_diff:.2f}%')\n",
    "\n",
    "        plt.scatter(X, Y, c=solution_w, cmap='jet')\n",
    "        plt.xlabel(\"Length (m)\")\n",
    "        plt.ylabel(\"Width (m)\")\n",
    "        plt.colorbar().set_label('w (m)')\n",
    "        plt.title(\"Mindlin Plate\")\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.show()\n",
    "\n",
    "def plot_loss2(model, loss):\n",
    "    x = range(len(loss))\n",
    "    plot_generic(x, loss, \"Iterations\", \"Residual Loss\", \"Loss\", log=False)\n",
    "\n",
    "\n",
    "def internalPoints(length, scale, steps):\n",
    "    z = torch.linspace(-length, length, steps) * scale\n",
    "    lower_bound = torch.min(z)\n",
    "    upper_bound = torch.max(z)\n",
    "\n",
    "    dz = (upper_bound - lower_bound) / (steps-1)\n",
    "    new_start = dz/2\n",
    "\n",
    "    new_coords = torch.linspace(lower_bound + new_start, upper_bound - new_start, steps-1)\n",
    "    return new_coords, dz\n",
    "\n",
    "\n",
    "def plot_generic(x, y, xlabel, ylabel, title, log=False):\n",
    "    with torch.no_grad():\n",
    "        plt.scatter(x, y, c='red')\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        #plt.xlim(left=-0.5, right=0.5)\n",
    "        #plt.ylim(bottom=-0.5, top=0.5)\n",
    "        plt.title(title)\n",
    "        if log:\n",
    "          plt.yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def getData(num_points, grad=False):\n",
    "  # Number of points\n",
    "  num_points = 80\n",
    "  # Generate random values sampled from a uniform distribution in the range [0, 1]\n",
    "  x = torch.rand(num_points, 1, requires_grad=grad)\n",
    "  x = x * (0.49 - (-0.49)) + (-0.49)\n",
    "  y = torch.rand(num_points, 1, requires_grad=grad)\n",
    "  y = y * (0.49 - (-0.49)) + (-0.49)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde = Pde(E=10920, nu=0.3, h=0.1, k=5/6)\n",
    "\n",
    "iterations = 100001\n",
    "report_step = 1000\n",
    "\n",
    "total_time = []\n",
    "total_error = []\n",
    "iterations_list = []\n",
    "exp_accuracy = []\n",
    "\n",
    "\n",
    "best_metric = 20\n",
    "for j in range(1):\n",
    "  residual_loss = []\n",
    "  rel_error = []\n",
    "  best_avg_rel_error = 100\n",
    "  timer = 0.0\n",
    "  rel_diff = 100.0\n",
    "\n",
    "  patience = 150\n",
    "  epochs_without_improvement = 0\n",
    "  dV =0.0\n",
    "\n",
    "  input_dim = 2  \n",
    "  embedding_dim = 32  \n",
    "  hidden_layers = [32, 32, 32, 32]  \n",
    "  output_dim = 5 \n",
    "  model = Pinn(input_dim, embedding_dim, hidden_layers, output_dim, sigma=10.0, lr=1.0e-3)\n",
    "\n",
    "  iteration_time_start = time.time()\n",
    "  dV = 0.0\n",
    "  fem = 10.0\n",
    "\n",
    "  for i in range(iterations):\n",
    "\n",
    "        model.optimizer.zero_grad()\n",
    "       \n",
    "        x, y = getData(num_points=40, grad=True)\n",
    "\n",
    "        resloss, derivloss = pde.getLoss(model, x, y)\n",
    "\n",
    "        loss = resloss + derivloss\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        #model.scheduler.step(loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            residual_loss.append(resloss.item())\n",
    "            #print(residual_loss)\n",
    "          \n",
    "            if i % report_step ==0:\n",
    "                print(f'rel_diff {rel_diff}')\n",
    "                print(f'res loss {resloss.detach().numpy()}')\n",
    "                plot_loss(model=model, comp=fem)\n",
    "                plot_loss2(model=model, loss=residual_loss)\n",
    "\n",
    "            # Checkpoint\n",
    "            # if rel_diff > 10:\n",
    "            #   if int(rel_diff) < int(best_metric):\n",
    "            #       best_metric = rel_diff\n",
    "            #       # Save the model\n",
    "            #       best_avg_rel_error = best_metric\n",
    "            #       torch.save(model.state_dict(), 'best_model.pth')\n",
    "            #       print(f'Saving model with best rel_diff: {best_metric}')\n",
    "            # else:\n",
    "            #   if rel_diff < best_metric:\n",
    "            #       best_metric = rel_diff\n",
    "            #       # Save the model\n",
    "            #       best_avg_rel_error = best_metric\n",
    "            #       torch.save(model.state_dict(), 'best_model.pth')\n",
    "            #       print(f'Saving model with best rel_diff: {best_metric}')\n",
    "\n",
    "  with torch.no_grad():\n",
    "      plot_loss(model=model, comp=fem)\n",
    "      print()\n",
    "      plot_loss2(model, residual_loss)\n",
    "      print()\n",
    "      print(f'Experiment {j}, time: {total_time[-1]}, best_accuracy: {total_error[-1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
